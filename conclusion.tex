%% ----------------------------------------------------------------------------
% BIWI SA/MA thesis template
%
% Created 09/29/2006 by Andreas Ess
% Extended 13/02/2009 by Jan Lesniak - jlesniak@vision.ee.ethz.ch
%% ----------------------------------------------------------------------------

\chapter{Conclusion}
The aim of this thesis was to analyze the relation between segmentation accuracy and annotation effort. We understood the requirement of iterative semi-interactive annotation to obtain best results for given annotation budget. The semi-interactive process can direct us to obtain best results instead of using our effort arbitrarily in semi-supervised learning. Another thing we tried, were different methods to compensate for training data. We started with the use of pre-trained network using fully annotated objects. It was observed to get good results even with 2 images used for training. Then, we used the Bayesian framework to use variational methods and observe a boost in performance. This motivates us to make use of variational methods to compensate for the lack of data. Finally, we were able to use CNN with variational methods to obtain better results. The simple modification of cross entropy loss for scribbles made it possible to use pre-trained fully convolutional networks. This opens a path to use networks pre-trained on million of images in semi-supervised learning. We also observed that for our given data, RF worked better than CNN in the case of fewer data. This can help in making a choice of method to be used for given annotation budget. \par
Currently, the use of total variation may not be common because of its implementation. It is considered as a post-processing step and not as a solution in a Bayesian framework. In literature, we found recent papers trying to combine CNN and variational methods to obtain better results. They also showed that it can simplify the complexity of CNN and is able to utilize prior information. Ranftl \cite{ranftl:2014} did this by adding total variation as an inference layer and solved the final problem using bilevel optimization \cite{ochs:2015}. This approach has a benefit to learn appropriate regularization parameter ($\lambda$) for total variation. The other approach related to implementation is to split the iterations used to optimize final problem (mentioned in section 3.2) as separate layers in the neural network. This is termed as Primal-dual network and described in Riegler et al. \cite{riegler:2016}. Each iteration can be implemented as one additional layer in networks. We tried to couple these two approaches and came up with the idea of having variational neurons similar to convolution filters. These variational neurons will take the image as input and produce a TV regularized output. Instead of learning $\lambda$, we can use multiple values of $\lambda$ and combine then using a CNN. This also opens a way to remove drawback of finding appropriate $\lambda$, while using variational methods. Due to time restriction, we were not able to complete this, but this idea can provide an option to get best out of multiple methods in a simple unified way. 