%% ----------------------------------------------------------------------------
% BIWI SA/MA thesis template
%
% Created 09/29/2006 by Andreas Ess
% Extended 13/02/2009 by Jan Lesniak - jlesniak@vision.ee.ethz.ch
%% ----------------------------------------------------------------------------

\chapter{Conclusion}
The comparison of results for RF and CNN shows that both methods have their advantages and disadvantages. RF can outperform CNN under certain conditions especially for low amount of training data. The amount of training data significantly effects the results for both methods. The aim of this thesis was to analyze the relation between segmentation accuracy and annotation effort. We can conclude that the results do not always improve with increment of training data. We observed that with arbitarily addition of scribbles, the accuracy did not improve in many cases. We need to increase not only the quantity of training data, but also the quality of it. Similarly, for training CNN with full annotations in chapter 2, we observed a fall in accuracy with increment of training data.
In case of semi-supervised learning, we understood the requirement of iterative semi-interactive annotation to obtain best results for given annotation budget. The semi-interactive process can direct us to obtain best results instead of using our effort arbitrarily in semi-supervised learning. Another thing we tried, were different methods to compensate for training data in our task of segmentation. We were able to use a pre-trained network using non-microscopic images and fine-tune it to produce good results for microscopic images. In addition to that, we used the Bayesian framework to use variational methods to compensate for training data. The comparison of different cost functions showed precautions to be taken while using different cost functions. \par
Another major contribution of this thesis was the success of fine-tuning a pre-trained network using partial annotations. The simple modification of cross entropy loss for scribbles made it possible to use pre-trained fully convolutional networks. This opens a path to use networks pre-trained on million of images in semi-supervised learning. 
Currently, the use of total variation may not be common because of its implementation. It is mainly considered as a post-processing step to improve results. The VIP can be used more, if it can be easily implemented and used. Recently, we can find a lot research trying to combine CNN and variational methods to obtain better results. Some papers show that it can also simplify the complexity of CNN. Ranftl \cite{ranftl:2014} did this by adding total variation as an inference layer and solved the final problem using bilevel optimization \cite{ochs:2015}. This approach has a benefit to learn appropriate regularization parameter ($\lambda$) for total variation. The other approach related to implementation is to split the iterations used to optimize final problem (mentioned in section 3.2) as separate layers in the neural network. This is termed as Primal-dual network and described in Riegler et al. \cite{riegler:2016}. Each iteration can be implemented as one additional layer in networks. \par 
Due to lack of sufficient time, we were able to theorotically couple these two approaches and came up with the idea of having variational neurons similar to convolution filters. These variational neurons will take the image as input and produce a TV regularized output. Instead of learning $\lambda$, we can use multiple values of $\lambda$ and combine then using a CNN. This also opens a way to remove drawback of finding appropriate $\lambda$, while using variational methods. Due to time restriction, we were not able to complete this, but this idea can provide an option to get best out of multiple methods in a simple unified way. 