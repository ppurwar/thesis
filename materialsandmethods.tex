%% ----------------------------------------------------------------------------
% BIWI SA/MA thesis template
%
% Created 09/29/2006 by Andreas Ess
% Extended 13/02/2009 by Jan Lesniak - jlesniak@vision.ee.ethz.ch
%% ----------------------------------------------------------------------------
\newpage
\chapter{Method for image segmentation}
\section{Bayesian Formulation}
We model our image segmentation problem as a Bayesian inference problem.
Let us cosider an observed image, \mat{I} and labeled or segmented groud truth, \mat{S}, the joint probabilty can be defined as:
\begin{equation*}
\p(\mat{I}, \mat{S}) = \p(\mat{S}) \p(\mat{I}|\mat{S}) \eqcont
\end{equation*}
and applying Bayes theorem,
\begin{align*}
\p(\mat{S}|\mat{I}) \, & = \frac{\p(\mat{S}) \p(\mat{I}|\mat{S})}{\p(\mat{I})} \\
						& \alpha \, {\p(\mat{S}) \p(\mat{I}|\mat{S})}
\end{align*}
The left hand side is the probability of \mat{S} given the image \mat{I}, is called the
posterior probability. \p(\mat{S}) is the prior probability of \mat{S}. The Maximum a posteriori (MAP) estimate, $\mat{S^*}$ can be calculated as follow:
\begin{equation}
\mat{S^*} = \arg\max_{\mat{S}}({\p(\mat{S}) \p(\mat{I}|\mat{S})}) \eqend
\end{equation}

The above problem can as well be stated as an energy minimization problem by
writing Equation 3.1 in terms of energy:
\begin{align*}
\funop{E}(\mat{S}) &=\, - \, \log(\p(\mat{I}, \mat{S})) \\
&= -\, \log(\p(\mat{I}|\mat{S})) - \log(\p(\mat{S})) \\
&= \funop{E_d}(\mat{I}, \mat{S}) + \funop{E_r} (\mat{S})
\end{align*}
The total energy, $\funop{E}$, that we want to minimize can be considered as linear combination of data term, $\funop{E_d}$ and prior term (or regularization), $\funop{E_r}$. This modifies calculating MAP estimate to:
\begin{equation*}
\mat{S^*} = \arg\min_{\mat{S}}({\funop{E_d}(\mat{I}, \mat{S}) + \funop{E_r} (\mat{S})}) \eqend
\end{equation*}

\section{Data energy term or Maximum likelihood cost function}
The data term can be related to the likelihood of getting a label (foreground) at each pixel using any prediction model. The prediction model will provide a value between 0 and 1, corresponding to probability of that pixel being foreground. The data term is generally used as a pixelwise product of expected mask and some pixelwise function of probablility. In general, there is use of a linear function of probability mask obtained from prediction model as given below:
\begin{equation*}
\funop(E_d)(\p, \vec{u}) = u(1-2\p) \eqcont
\end{equation*}
which ensures a negative cost for foreground pixels. 

\section{Prediction models}
\subsection{Random Forests}
Explain random forests
\subsection{Convolutional Neural Networks}
Neural networks \newline
Explain transfer learning also.

\section{Prior energy term using Total Variation}
Explain total variation regularisation term.


\chapter{Implementation}
\section{Solving variational problem using ASB}
Solution of ASB problem
\section{Random Forest}
Feature selection and training of Random forests.
\section{Architecture of CNN used}
Training of CNN \newline
Loss function for scribbles
